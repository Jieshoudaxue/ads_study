自动驾驶学习
前言：既然看准这个方向，那就做到最好，目标是成为自动驾驶系统架构师，全栈工程师（行业急缺），利用好自身善于学习的品质，apollo平台和公司平台，坚持三年，日积月累，达到质变，再上一个台架！！！
学习的核心是编程基础+自动驾驶业务，要单独学，也要融合学，最终实现算法到工程的融会贯通。要找大牛，要找好资料，又要干，又要学，最重要的是持之以恒，是持久战！！！


智能驾驶入门课程（优达+百度）

1.10.2 你将学到什么
（1）Apollo主要组成：HD Maps，Localization，Perception，Prediction，Planning，Control
（2）HD Maps支持着软件栈几乎其它所有模块；
（3）Localization课程讲解汽车依赖GPS，IMU，激光，加上HD Maps，实现个位数厘米级定位；
（4）Perception课程讲解使用卷积神经网络的深度学习，分析camera，激光和雷达数据；
（5）Prediction课程讲解用于预测车辆和行人可能如何行动的几种方式，比如递归神经网络；
（6）Planning课程讲解，如何将预测和路线相结合，生成车辆轨迹，规划是无人驾驶技术中最困难的部分之一，本课程将讲解Apollo的几种规划方法；
（7）Control课程讲解，如何使用方向盘，油门和刹车执行规划轨迹，本课程将讲解几种不同的控制器，从简单到复杂，控制效果将越来越好；
（8）本课程结束，学员将了解自动驾驶技术的基本工作原理；
1.11.3 什么是无人驾驶
（1）无人驾驶的优势：不会疲劳，不需要从头学习，安全性大增；普及后，人类不再需要停车场和宽阔的马路，城市将还给人；
（2）6级无人驾驶：
0级：完全有人开；
1级：定速巡航；
2级：ACC，车道保持，AEB；
3级：有条件的自动化，但驾驶员要随时接管，比较尴尬的级别；
4级：特定区域内的无人驾驶，俗称地理围栏；
5级：完全的无人驾驶，驾驶安全性高于人类；
1.15.7 参考车辆和硬件平台
（1）需要有一辆可以通过can总线控制刹车，方向盘和油门的车，据说林肯的车的can信号是开放的；
（2）车辆需要安装：多个camera，多个雷达，激光雷达，GPS，IPC（工业电脑），imu设备：
camera：获取车周围影像，能得到颜色信息，可以看交通灯；
雷达radar：便宜好用，可以准确获取车周围较远物体的距离和速度（多普勒效应），且对环境的要求不高，雾霾天气也可用。对金属物体探测效果好，但噪点多，可以用来探测周边车和高速护栏！
IMU：通过速度，加速度传感器，测量车自身的位置和运动；
激光雷达lidar：通过发射激光束，扫描周围环境，获取点云，黑暗环境，优势明显，但比较贵，且对环境要求搞，雾霾天不行；
GPS：获取全球位置和绝对时间，但密闭空间会失效，定位精度米级；
补充：radar的波长是毫米级，lidar的波长是纳米，波长越长，越能穿过障碍物；
1.16.8 Apollo开源软件架构（基本都是类似的）
（1）主要分为三层：实时操作系统RTOS，运行时框架和应用程序模块层
（2）Apollo RTOS是ubuntu改了内核，线程调度上加入实时性，这样一来兼顾了实时和已用，比qnx好点；
（3）运行时框架是ROS的定制化（类似mfr），定制包括通信采用共享内存，效率高；去中心化，增加公共域节点，避免主节点挂，系统全挂的单点故障问题（ROS2已经实现）；采用protobuf替换原生的ROS消息，优化数据兼容，避免一块升级，整个全升级的问题（后向兼容）；
（4）应用程序模块包括MAP Engine，Localization，Perception，Planning，Control，End-to-End，HMI（人机）
补充：猜测ROS2都实现了Apollo的这些升级，但是普及度不如ROS1，需要调研补充？？？
1.17.9 Apollo云服务
（1）云服务是一套软件：HD Map（高精地图集），Simulation（仿真系统，momenta最欠缺的），Data Platform（数据平台），security，OTA，DuerOS（智能语音系统）
（2）仿真系统是云服务的重点，内部有大量数据集，支持用户构建虚拟的特定场景来检验算法，自带评分系统，甚至达到输出可视化；
（3）数据是自动驾驶的根基，ApolloScape数据集涵盖各种场景，已经对外开放；
补充：了解各个模块，可以访问apollo.auto

2.22.3 高精地图VS传统地图
（1）高精地图包含大量的驾驶辅助信息，最重要的是道路网的精确“三维表征”，比如红路灯，交通牌，车道，转向标识，因此高精地图提供了大量语义信息；
（2）高精地图最重要的特征是精度，手机GPS精度为米级，高精地图可以使定位达到米级；
（3）高精地图要求具有很高的实时性，能反应最新的道路信息
（4）高精地图是为自动驾驶系统专门制作的地图，而普通的导航地图是为人看的；
2.23.4 高精地图与定位
（1）车辆有高精地图，就需要在该地图上进行自定位，即弄清楚自身在地图上的位置；
（2）车辆自定位的原理类似于拼图游戏，车辆使用自身传感器寻找地标，比如路口的花坛，然后用地标与高精地图进行匹配；
（3）收集地标可以通过camera和lidar，即图像和点云。
（4）在匹配之前，需要进行预处理（去除不准确和质量差的数据），坐标转换（基于标定，将不同视角的数据统一为同一坐标系）和数据融合（将多种传感器数据进行合并）；
2.24.5 高精地图与感知
（1）车辆传感器与人类的眼耳一样，同样会受到距离，光照条件和障碍物的限制。高精地图可以提高车辆的提前预判能力，即提前告知软件栈下一个路口的情况，即超视距能力；
（2）高精地图还可以帮助车辆构建感兴趣区域（ROI），缩小传感器搜索范围，节约算力，比如在路口找可停车标志；
2.25.6 高精地图与规划
（1）高精地图可以帮助车辆规划线路，避免拥堵，像传统地图一样，但高精地图可以帮助车辆进行车道lane级别的规划；
（2）高精地图可以帮助车辆识别车道的确切中心线，帮助车辆走在车道中间；
（3）高精地图可以帮助车辆在有低速标志，人行道和减速带的情况下，提前减速。甚至在有障碍物，不得不绕行的情况下，可以帮助车辆缩小选择范围；
2.26.7 Apollo高精地图
（1）Apollo高精地图采用OpenDRIVE格式（一种行业制图标准，另外还有NDS标准，但是比较重），并进行了改进；
（2）百度研发了一套自动化制图的工具，使得90%的流程可以自动化；
（3）制图可分为五步：数据采集，数据处理，对象检测，手动验证和地图发布；
（4）由于地图制作的高成本和高频更新问题，一般都会采用众包形式，即向公众发布数采工具，公众可通过手机，车载摄像头等采集数据，然后统一传给百度Apollo云端；

3.29.1 定位简介（where we are）
（1）定位是自动驾驶算法模块的第一块，无人车只有知道自己当前的位置，才能知道下面怎么走
（2）定位有多种方式，最常用的是GPS，GPS是使用最广泛的全球卫星定位系统，即GNSS，还有GPS的精度增强技术RTK；还有基于IMU传感器的惯性导航；基于lidar与高精地图的定位；基于视觉的定位；
（3）虽然定位方式很多，但各有优缺点，实际情况是采用多种定位技术相互补充，最终实现厘米级定位；
3.31.3 GPS+RTK
（1）GPS采用三角测量，即平面上，三个圆的焦点是唯一的；空间里，三个球的焦点是唯一的；
补充：因此，理论上只要三颗卫星，就能实现全球定位，但一般情况下都采用30颗以上的卫星，实现更高精度的全球定位；
（2）GPS接收器并不直接计算其与卫星的距离，而是通过计算信号传播时间，然后乘以光速，得到两者距离，因此存在一定误差；
Distance = c * delta（ns）
（3）RTK技术是GPS的补充技术，通过地面上确切位置的基站，与GPS卫星计算距离，得到距离误差，然后传给无人车用于定位矫正，可以使定位达到10厘米级别；
（4）GPS定位很方便好用，但弱点是在密闭空间或有山，大楼遮挡，林荫路的情况下，GPS信号不稳定，定位误差极大，可以达到数十米（此时RTK技术也是失效的）；而且GPS信号的更新频率为10Hz左右，对于高速移动的车辆来说，太慢了！
（5）GPS有三大功能：定位，授时和测速
3.33.5 惯性导航
（1）即通过IMU传感器，得到加速度（也可以得到角速度，内有陀螺仪），加上车速（轮速计），然后通过经典的运动学公式，即可得到一段时间后汽车的位置；
（2）IMU具有极高的更新频率，可以达到1000Hz，因此可以提供接近实时的位置信息；
（3）惯性导航的弱点是误差会随着时间的延长而累积，因此不能长时间使用，多数情况下，它都会与GPS共同使用，相互补充；
（4）GPS+惯性导航也不能满足所有情况，比如长时间在大楼间行驶，在山间行驶，在隧道行驶，GPS得不到更新，定位仍然会失败；
补充：momenta的imu频率是100hz；
补充：momenta的highway要求车辆行驶在高架高速等开阔路况，因此定位靠GPS+IMU即可，再加上车载导航和视觉巡线，就能实现highway，不需要高精地图辅助定位，因此highway没有高精地图；
3.35.7 激光雷达定位
（1）激光雷达扫描周围物体得到点云，然后与预先存储的高精地图进行连续匹配，可以得到汽车在高精地图中的精准定位和行驶方向；
（2）三种lidar定位算法：
迭代最近点（ICP）：具体原理没听明白？？？
直方图滤波算法（SSD）：也称误差平方和算法，即将扫描的点云划过图中的每个位置，并计算误差平方和，越小说明匹配的越好；
卡尔曼滤波算法：基于预测，然后进行比较，具体没听明白？？？
（3）lidar定位的优势是稳定，不受外界条件限制；弱点是高精地图难以构建，很难一直保持最新，而且地图都会包含大量的瞬态元素，即行人和车，下一次路过，他们就都没有了；
3.36.8 视觉定位
（1）单纯的视觉定位非常困难，一般依赖三维地图，优点是摄像头便宜，缺点是缺乏三维地图；
3.37.9 Apollo定位
（1）Apollo使用GPS，IMU，lidar，radar和高精度地图的融合定位方案，确保准确和稳定；
（2）Apollo使用GPS输出当前的位置和速度，lidar输出当前的位置和方向，IMU惯导预测下一步位置，速度和方向，使用卡尔曼滤波将这些融合在一起；
（3）Apollo卡尔曼滤波分为两步，IMU的惯导用于预测，GPS+lidar用于更新，具体就不知道了？？？

4.40.1 感知简介（what we see）
（1）人类使用眼睛观察车况，做出判断，无人车使用camera，lidar和radar观察周围，尤其是camera；
（2）从视频和图像中提取信息，就需要计算机视觉技术，其中涉及到机器学习，卷积神经网络（CNN）等大量火热的人工智能相关的技术
4.42.3 计算机视觉
（1）camera图像只是红绿蓝三色的集合值，准确说是深度为3的二维色层，计算机视觉就是负责从中提取出信息的技术；
（2）无人车感知模块的四个任务：
Detection：检测，即从环境中找出物体；
Classification：分类，即明确物体是什么；
Segmentation：语义分割，即将图像中的每个像素按语义进行分类，比如哪些像素属于天空，哪些属于树，哪些属于车；
Tracking：跟踪，即随时间的移动，跟踪某个物体，比如车，人；
（3）这四个任务基本都依赖卷积神经网络CNN，其特别适合无人车感知；
4.44.5 lidar点云图
（1）lidar通过自身发射的激光束，获得物体表面的点云图，比起二维的camera图像，可以轻松获得距离和高度数据；
（2）无人车感知模块同样可以根据lidar点云图进行检测，分配，跟踪和语义分割，这同样属于计算机视觉技术的一部分；
补充：lidar点云感知是业界难题
4.49.10 检测，分类，跟踪和语义分割四大任务
（1）目前实现检测和分类任务的经典神经网络模型是R-CNN，其变体有Fast R-CNN和Faster R-CNN，其中YOLO是其中使用较多的一种模型；
（2）对不同帧有的对象进行跟踪，需要提取对象特别的特征值，然后对特征值进行匹配，确定对象；
（3）语义分割依赖一种特殊的CNN，即全卷积网络FCN；
4.54.15 感知融合策略
（1）绝大多数情况下，感知依赖camera，但通过只有camera，lidar和radar加起来才能获得全方位的准确可靠的感知结果，即传感器融合；
（2）用于传感器融合输出的主要算法是卡尔曼滤波，分为两步，即预测和更新；

5.57.1 预测简介（how the environment will change）
（1）无人车处在运动过程中，因此需要预测其它运动体的行为，如旁边车辆变道或加塞，只有知道了周围环境的实时动态，才能有效的规划自己的行为；
（2）预测有三大要求：
实时性，即要一直不断的更新预测结果；
准确性，即预测一定要正确，因为一旦预测失败，比如后方车辆突然超车，而没有预测到，极有可能发生侧撞；
自学习性：预测算法必须要能自我学习，自我提升，因为场景是一直变化的，不可能为每个场景开发静态模型；
5.60.4 两种预测方式
（1）预测有两种模型，一种是基于模型的，一种是数据驱动的，前者利用了人类的各种知识，直观好理解，后者基于机器学习，能随着数据的增加不断进化，更有效；
5.61.5 Apollo基于车道序列的预测方法
（1）运动体的轨迹可能非常凌乱多变，因此需要简化预测难度；
（2）Apollo将道路分为多个区域，每个区域都能描述车辆运动的特征，比如丁字路口的右转区，直行区，我们有理由相信处在右转区的车辆，是要进行右转动作；
（3）于是轨迹预测简化成了车辆在不同区域内的转换，车辆行为变成了一组车道序列；
（4）人类预测运动体的行为是根据其一段时间内的状态变化，即朝向，速度和加速度，无人车预测运动体的行为也是如此；
（5）无人车根据运动体一段时间内的状态变化，预测运动体如何变化行驶区域，并规划出若干轨迹，然后根据运动体当前的状态限制，排除掉不可能的轨迹，最终实现运动体的轨迹预测；
（6）现在我们知道运动体的初始车道区域和一段时间内的状态，如果有一个数据驱动的模型，接受这些输入，然后输出运动体可能的车道序列，并给出统计概率，我们就能实现数据驱动的预测算法。这个数据驱动的模型就是递归神经网络RNN；
5.64.8 递归神经网络RNN
（1）RNN是一种利用时间序列数据的预测方法
（2）RNN是一组神经网络，一个单元为MLP，将一组时间序列数据分别传入RNN的不同MLP单元，每个MLP输出序列中下一个元素的预测，并把这个预测结果复制一份给旁边的MLP单元，除了第一个MLP，每个MLP都接收原始输入和前一个MLP单元的输出，并进行预测；
（3）由于前一个MLP单元的输出是当前MLP的输入，因此称为递归神经网络；
5.65.9 RNN在Apollo目标车道预测的应用
（1）Apollo设计了两个RNN模型，一个用于接收车道序列，另一个接收运动体状态序列，将两个的预测结果输出到另一个神经网络用于预测未来的车道序列，其中概率最大的那个就是我们的预测结果；
5.66.10 轨迹生成
（1）一旦我们预测到车道序列，我们就可能根据车道序列预测出轨迹
（2）直观的思路是列出可能的轨迹，然后根据运动体状态进行排除。实际上我们可以根据运动体初始状态和最终状态，拟合多项式进行预测，具体不懂？？？

6.68.1 规划简介（how we move）
（1）无人车完成规划需要高精地图，定位和预测
（2）规划分为两步，或者叫两层：
高一层是路由/路线导航，即解决如何从地图中的A点到B点，依赖车载地图，由导航模块完成；
低一层是轨迹规划，让无人车避开地图上没有显示的车，人和障碍物等，生成“免碰撞的”“舒适的”“可行驶的”轨迹，由规划模块完成。可行驶轨迹由一系列点定义，每个点由三维信息标识，即2D坐标和何时应抵达那个点的时间戳；
6.72.5 路由/路线导航
（1）路由是应用非常普遍的技术，基本的原理都是先将现实世界的地图抽象为节点和带权重的边的“图”，然后应用搜索算法找到合适的路径；
（2）成熟的算法有知名的迪杰斯特拉算法，即解决从顶点出发到其余顶点的最短路径；
6.76.9 可行使轨迹
（1）轨迹是由一系列的点组成，每个点都有自己的2D坐标和到达时间戳。为了到达这个点，一般也需要确定到达该点时车的速度；或者先确定到达该点时车的速度，也就能得出到达该点的时间；
（2）生成轨迹要与预测模块相结合，确保轨迹上的任何位置都是没有被占用的，避免碰撞；
（3）轨迹上的速度和方向的变化必须是平滑的，保证乘客的舒适性；
（4）轨迹必须是可行驶的，不能是车辆做不到的，比如高速情况下，突然掉头！
（5）轨迹必须是合法的，比如不占用应急车道，不逆向行驶，不超速行驶！
6.79.11 评估轨迹
（1）规划出的轨迹可能有多条，可通过成本函数进行选择，成本包括：
安全性：偏过中心线，到了逆向车道，增加危险；超速；
平滑性：轨迹曲率和加速度过大，乘客不舒服等；
（2）不同场景下评估轨迹的成本函数可能不同，比如parking和highway；
6.80.13 用于轨迹生成的几种工具
（1）Frenet坐标：横轴是车辆相对于道路中心线的偏移；纵轴是车辆在道路中的行驶距离，表示轨迹点位置；
（2）ST图：横轴是时间，纵轴是车辆在道路中的纵向位移。ST曲线的斜率是车辆纵向速度；
（3）ST和SL轨迹：轨迹的每个点都是三维信息点，即2D坐标+时间，可以将三维问题分解为两个单独的二维问题：
ST轨迹：横轴是时间，纵轴是车辆在道路中的纵向位移，即车辆相对于时间的纵向位移；
SL轨迹：横轴是纵向位移，纵轴是车辆相对于道路中心线的横向偏移，即车辆相对于纵向位移的横向偏移；
6.81.14 两种轨迹生成方法
（1）路径-速度解耦规划：先规划路径，然后规划速度，最后得出三维轨迹（2D位置+时间）
（2）Lattice规划：先把问题分解为ST和SL规划，然后直接合成三维轨迹。
6.82.15 路径-速度解耦规划
（1）路径生成是先将前方道路分成一系列单元格，然后在每个单元格里面进行随机采样，接着将连接多个单元格内的点生成候选路径；
（2）通常每个单元格内点很多，因此候选路径也很多，所以还需要使用成本函数进行评估筛选，最终得到规划路径；
（3）路径规划完成后，是速度规划，需要借助ST图，并需要将ST图离散为单元格，每个单元格内纵向位移不变，因此速度也就是唯一的，其会简化速度规划的难度；
（4）有了ST图后，先将预测模块输出的轨迹障碍（车人物）在图中以矩形画出，得到一张被各种矩形阻挡的ST图；
（5）根据ST图，画出多条位移时间曲线，并利用优化引擎选择最佳曲线，由于单元格内速度不变，也相当于是速度曲线。
（6）优化引擎会考虑车辆，法律，道路等多种因素，进行复杂的运算，具体不知？？？
（7）由于使用了离散化，因此我们得到的路径和速度规划曲线都是折线，需要经过二次规划拟合成平滑的曲线，二次规划比较复杂，具体不知？？？
补充：猜测应该有路径规划和速度规划合成这一步，并算出到每个点的时间，然后再拟合成平滑的轨迹，需要调查？？？
6.87.20 Lattice规划
（1）首先把车辆的初始状态投射到ST和SL图中，然后预测车辆的终止状态，一般分为巡航，跟随和停止，然后为每个初始状态到终止状态构建一条曲线；
（2）使用成本函数对多条曲线进行评估，然后找出最合适的；
（3）得到ST和SL规划后，将其合成，可得到轨迹；
补充：猜测会根据ST图算出每个点的速度，从而控制车辆？？？

7.93.1 控制简介（how to control car）
（1）对于车辆的控制主要通过油门，刹车和方向盘，跟人类驾驶一样；
（2）控制的目标是让车辆沿着规划确定的轨迹前行，要求准确，满足安全性；平稳，满足舒适性；具有可行性，比如车辆不能原地转90度；
（3）总结起来就是，使用可行的控制输入，最大限度降低与目标轨迹的偏差，最大限度提高乘客的舒适度；
（4）三种控制策略：
PID：比例积分微分控制
LQR：线性二次调节器
MPC：模型预测控制
7.95.3 控制流程
（1）规划模块给出的轨迹其实是一系列目标点，每个点都有位置和参考速度；
（2）车辆通过自身传感器感知自己的位置和速度，然后与轨迹点进行比较，从而得到目标轨迹与实际轨迹的偏差；
（3）控制的输出是转向，加速和制动的值，通过不断的调整这三个输出值，从而让车辆自动地沿规划轨迹行驶；
7.97.5 PID控制
（1）PID控制属于经典控制理论，典型的负反馈控制，应用最广泛，也最好用；
（2）其中P是比例，聚焦于当前，矫正当前误差；i是积分，聚焦于过去，消除累积误差；d是微分，也就是速率，聚焦于未来，保证稳定性；
（3）PID控制最主要的就是调整PID的三个参数；
（4）PID控制的优点是成熟可靠，缺点是无法应对复杂系统的控制，比如很难兼顾车辆的横向和纵向控制；
补充：PID控制的详细解释可以看百度百科介绍，挺全的
https://baike.baidu.com/item/%E6%AF%94%E4%BE%8B%E7%A7%AF%E5%88%86%E5%BE%AE%E5%88%86%E6%8E%A7%E5%88%B6/22010564?fromtitle=PID%E6%8E%A7%E5%88%B6&fromid=4748784&fr=aladdin
7.101.9 线性二次调节器LQR
（1）PID属于经典控制理论，LQR就属于现代控制理论了，使用的是矩阵运算，具体就不追究了？？？
7.102.10 模型预测控制MPC
（1）这个更复杂，按下不表？？？


智能驾驶进阶课程（百度）

1 自动驾驶行业概述
1.1 自动驾驶行业概述
（1）自动驾驶的未来很好，城市还给人，交通安全大增，交通效率大增
（2）技术迭代的周期大大加快，自动驾驶从萌芽到成熟，正好在我们的职业生涯里，最晚的预期是2040年普及，前景和钱景都不错
（3）汽车的未来是电动化，智能化，自动驾驶化；
1.2 Apollo开源模块讲解
（1）第一部分大概梳理了技术栈，从传感器，高精地图，定位，感知和预测，pnc，最后还有百度云端服务；
（2）第二部分讲了汽车安全，安全分为safety和security，前者指车自身的问题，由ISO26262设定最高标准；后者指车能防攻击或者防误用能力，可以理解为预期功能安全和信息安全；
（3）第三部分讲了apollo从1.0到3.0的技术迭代，1.0循迹，1.5车道保持，2.0城市简单道路，2.5城市高架，3.0园区内量产无人小车
补充1：汽车安全相关概念
a.人身安全：指的是规避汽车操作对驾驶员或者路人或周边车辆内人员（注意不仅是驾驶员）的人身危害。人身安全中包括了功能安全、预期功能安全和用户使用安全。
b.功能安全：指的是不存在由电子电气系统的功能异常表现引起的危害而导致不合理的风险。功能安全是在汽车电子电气技术基础上发展而来的一项安全技术，对应的标准为ISO 26262。
c.预期功能安全：指的是规避由于功能不足、或可合理预见的人员误用所导致的人身危害。预期功能安全技术属于智能网联汽车技术的一部分，对应的标准为SOTIF ISO 21448。
d.信息安全：指的是规避重要信息泄露、被篡改、盗窃或遗失。信息安全同样属于智能网联汽车技术的一部分，对应的标准为ISO/SAE 21434和SAE J3061。
e.功能安全和预期功能安全的关系：功能安全用于解决电子电气失效对人造成的危害，而预期功能安全用于解决系统非故障原因对人造成的危害。ISO 26262标准中对电子电气系统的标称功能及性能没有要求，预期功能安全的存在弥补了这部分遗憾。
f.智能网联汽车操作安全性的“安全三剑客”：功能安全、预期功能安全和信息安全。功能安全解决【电子电器失效】对人造成的危害；预期功能安全解决【系统非故障原因】对人造成的危害；
补充2：预期功能安全：
a.随着汽车电气化时代的到来，人们发现并不是所有的车辆安全问题都源于系统错误和失效。很多时候，在复杂系统中，系统安全的问题来源于环境影响带来非预期的安全问题。对于自动驾驶系统来说，车辆由系统控制，控制效果的优劣是必须要考虑的因素。在传统汽车领域中，失败的表现往往源于系统的失效。在自动驾驶系统中则不然，即使系统不发生故障，也可能因为神经网络黑盒输出等因素的不确定性导致功能的偏离，造成交通伤害。这类非故障情况下因系统功能不满足预期而导致的安全风险就是预期功能安全要解决的问题。
b.因自动驾驶车辆运行场景条件的复杂性和未知性，自动驾驶功能即使满足设计要求，仍可能存在大量的安全运行风险。如何避免预期的功能所引发的安全风险，即为预期功能安全。预期功能安全的定义：不存在因设计不足或性能局限引起的危害而导致不合理的风险，也就是将设计不足、性能局限导致的风险控制在合理可接受的范围内
c.自动驾驶系统局限可分为几个部分：
感知- 目标场景考虑不周到，系统无法对环境做出正确响应
决策- 功能逻辑仲裁机制、算法不合理，导致决策出现问题
执行- 执行机构的输出与理想输出发生偏差，难以完美控制
d.SOTIF ISO 21448的SOTIF为Safety of The Intended Functionality，SOTIF，重点关注“预期的功能”的安全性，即：满足预期设计要求的功能所具有的安全水平。
补充3：apollo现在已经发布了7.0，并且在3.5版本进行了一次大的架构升级，可以作为新老的分界线

2 Apollo硬件开发平台
2.1 Apollo硬件开发平台
（1）自动驾驶开发的四步骤：
软件在环：基于模拟器，仿真平台测试运行自动驾驶算法软件，这块momenta非常弱，几乎没有，各模块只使用简单的ros回放进行自测；
硬件在环：基于必要的硬件平台，比如qnx，orin和mdc，嵌入式的跑自动驾驶算法软件，也就是momenta的IPC和IPD回放；
车辆在环：基于车辆平台，在封闭场所测试；
司机在环：基于实际道路，师傅测试使用，momenta的上车包括这里的车辆在环和司机在环两部分；
补充：momenta的开发比较激进，很多时候直接从软件在环蹦到了司机在环，然后靠集成同学去卷；
补充：wsbag2.0加上后面要弄的确定性调度，就是提升硬件在环的能力！
（2）自动驾驶汽车的硬件系统：除了前面入门课程中讲到的camera，radar，lidar，imu和gps，还提到了uss（超声波）：Ultrasound Scan超声波扫描，超声波系统主要由超声波传感器（USS）构成，由于探测距离非常短，且依赖声速测量，因此只能用于短距低速场景。
补充：uss主要作用是泊车、侧视辅助。
（3）自动驾驶的实现依赖多个传感器，各个传感器各有侧重，需要融合，这节课指出未来传感器的发展思路是传感器融合，比如将camera和lidar直接融为一体，并在内部添加专门的数据融合芯片（fpga，asic），然后直接输出融合后的数据，相关公司有美国的激光雷达公司AEye，国内的禾赛科技，和瑞萨等。这是创新的方向，而且与之前的工作相关
（4）除了上面第三点，还有车载联网设备T-box（v2x），车身域控制（TSN），汽车黑匣子（cfdi）也与自己的工作相关；
补充：https://developer.apollo.auto/devcenter/courseplaying_cn.html?target=2-247-266 这个硬件课程值得多看两遍！！！

3 百度Apollo定位技术
课程讲解无人车的定位技术，为学习者讲解无人车定位的概念、方法以及百度Apollo无人车定位的技术。
3.1 无人车定位技术入门
（1）定位不但是要获取车辆的位置，还包括车辆的姿态，速度，加速度，角速度等一系列信息。
（2）姿态指的是车辆相对xyz坐标轴的旋转信息；
俯仰：相对x轴的旋转
横滚：相对y轴的旋转
航向：相对z轴的旋转
（3）定位技术指标要求
精度，用误差均值表示，理想值：<10cm
鲁棒性，用最大误差表示，理想值：<30cm
场景，用可覆盖场景数表示，理想值：全天候
（4）三类定位技术：
基于电子信号定位：GNSS
航迹推算（惯性导航sins）：IMU是惯性测量装置，内部有加速度计和陀螺仪，前者获取车辆加速度，后者获取车辆角速度。加速度积分可以获取车速增量，角速度积分可以获取车辆相对xyz坐标轴的旋转增量，即姿态。有了加速度和角速度，加上车辆初始速度（来自轮速计），就可以计算车辆行驶距离。
环境特征匹配：Lidar（激光雷达定位），camera（视觉定位）
3.2 无人车定位基础知识
（1）先粗略讲解了一些基础知识（保留的ppt截图）：右手坐标系，二维旋转，三维旋转，欧拉角和四元数，三维平移，刚体位置和朝向。这些基础知识是下面坐标系变换的预备知识。
（2）粗略讲解了定位技术涉及的坐标系：地心惯性坐标系，地心地固坐标系，当地水平坐标系（ENU），通用横轴墨卡托投影（UTM），车体坐标系，IMU坐标系，相机坐标系，激光雷达坐标系），车体坐标系，IMU坐标系，相机坐标系，激光雷达坐标系
（3）最后用一幅图讲解了多种坐标系下数据的处理关系，最终得到用于定位的多个坐标系下的车辆数据，这些数据会传给感知和pnc。
UTM下的(x,y,z)
IMU姿态四元数
ENU下的速度
车体坐标系下的加速度
车体坐标系下的旋转角速度
车体姿态四元数
3.3 百度无人车定位技术
（1）概述介绍了几种定位技术（保留了截图）：
GPS定位，并列举了gps误差源
激光雷达定位，给出了算法框架
视觉定位，给出了算法框架
惯性导航定位，给出了算法框架
最后给出了组合定位技术算法框架
（2）更详细的资料见百度apollo的论文：
Robust and Precise Vehicle Localization Based on Multi-Sensor Fusion in Diverse City Scenes
ICRA，2018
知乎对这篇论文的详解：https://zhuanlan.zhihu.com/p/420325143

4 百度Apollo高精地图
课程介绍高精地图的定义，其在无人驾驶各个模块中的作用、高精地图的采集与生产以及Apollo高精地图的相关知识。
4.1 高精地图与自动驾驶的关系
（1）入门课程讲解了高精地图与定位，感知和规划的关系，其实高精地图也能辅助预测，比如旁边车辆走在地图中的左转车道，预测模块就可以认为他要左转；比如绿灯亮了，行人走在马路线上，预测模块就可以认为他要过马路；
（2）前面讲了预期功能安全，即指车能防攻击或者防误用能力，可以理解为预期功能安全和信息安全，由SOTIF ISO 21448约束，因此自动驾驶应该有一个安全模块（momenta没有！）。
目前车辆可能受到的攻击或影响有：
强磁场干扰imu测量；
破坏轮胎影响轮速计量；
人工加反射影响lidar；
假交通灯；
大功率假gps信号；
激光屏蔽lidar；
通过传感器探测结果与高精地图比对，高精地图可以帮助安全模块识别可能的攻击和影响。
（3）自动驾驶依赖仿真系统进行软件在环，而高精地图就是仿真系统的基础，仿真系统依赖高精地图构建测试场景；
（4）没有高精地图，就没有高可靠性的L3/L4自动驾驶
4.2 高精地图的采集和生成
（1）高精地图制作有三个路线方向：
基于lidar，目前无人驾驶公司制图多用这个
基于lidar和camera融合制图，目前的制图技术方向
基于camera，英伟达，宽凳，deepmotion号称可以做到纯视觉高精度制图
（2）不管是用lidar还是camera，都不能缺少高精度的定位系统，否则就无法知道制出来图的具体位置。
定位系统依赖：IMU，轮速计，GPS，lidar/camera；
IMU+轮速计：实时计算和预测车辆当前位置，前者提供加速度角速度信息，后者提供初始车速信息；
GPS+lidar/camera：gps定位融合视觉和点云定位，可以周期性的矫正IMU+轮速计的误差；
（3）高精地图精简计算模型：J = Q(z - h(m,x))
Q：优化方程
z：lidar扫描出的点（具体如何用数字表示不清楚）
h：预测扫描点位置的方程
m：lidar扫描到的点在地图的位置
x：无人车定位系统给出的当前位置
J：该公式的目标就是通过最小化J，计算得到m！
（4）当获取到当前的准确位置和lidar+camera提炼的数据后，就可以将新数据加入地图中
（5）有些公司号称利用简单的工具，类似手机，车载记录仪，就可以实现基于camera的高精地图众包采集。众包制图是好思路和发展的方向，但是单纯依赖这个很难有特别好的效果！
4.3 高精地图的格式规范
（1）通用的制图标准有NDS和OpenDrive，前者比较重，宝马用这个，后者由apollo采用
（2）OpenDrive中的几个元素：
section：道路被切分为众多的section，比如车道变多变少时
reference line：道路中间线，往上分别为lane1，lane2，lane3；往下为lane-1，lane-2，lane-3
Junction area：路口区域
4.4 业界的高精地图产品
（1）here以前属于诺基亚，后来卖给了宝马奔驰，他的制图流程是先用lidar做基础的地图，然后用基于camera的众包采集更新地图。这样极有精度的保障，又能做到实时更新。
（2）here的live map地图是分层的，从低向上依次是：
Road&Lane Model Layer：高精度的路网描述
Localization Model Layer：定位层，显示例如电线杆，公交站台，电话亭，大楼等固定物体的位置，帮助车辆定位
Activity Layer：活跃层，比如道路封闭维修，车祸
Analytics Layer：习惯层，统计该路段用户的驾驶习惯，帮助无人车驾驶起来更像人（以前没听过）
补充：由于地图采集的数据量非常大，在4G网络下，很难实时采集上传，因此目前都是先存下来，然后二次拷贝上传；
（3）MobileEye，做汽车ADAS起家，现在被英特尔收购，成为英特尔自动驾驶部门，也做地图。其地图主要采用视觉路线，发布了称之为REM（real time）的众包制图系统。
（4）做地图的还有：google waymo，Tomtom，
4.5 Apollo地图采集和制作方案
（1）apollo采用lidar加camera融合的方案，先采集beg数据，然后离线制作地图
（2）制图过程包括：点云拼接生成底图，基于深度学习的元素识别和分类，人工校准验证车道线，信号灯，标识牌
（3）apollo制作的高精地图其实是四个：用于定位的地图，描述各个元素的高精地图，还有基于高精地图生成的辅助规划地图和仿真地图
（4）apollo高精地图包含的元素（已经截图）：道路元素，路口元素，交通信号元素，逻辑关系元素，其它道路对象元素

5 百度Apollo规划技术介绍
本课重点简介Apollo路径规划策略及优化方式
5.1 motion planning（运动规划）概述
（1）规划在算法上，本质是搜索问题，在数学上，是找最优解问题；
（2）规划问题的描述或目标：safely and smoothly maneuver autonomous driving vehicle to the destination，即安全平稳的使无人车到达终点
（3）规划的输出是可行使轨迹，是路径加速度两种信息的融合，而是不只是一条path；
（4）规划在不同领域的解释及方法：
机器人领域：如何产生一条轨迹到达目标；
            RRT，A*，D* lite，Lattice Planning
控制领域：动态系统到达目标状态；
          MPC，LQR
人工智能：生成状态和动作的映射（Data Drive）
        Reinforcement learning，end to end imitation learning
补充：http://planning.cs.uiuc.edu/parl.pdf Steve Lavelle
（5）重要文献
Steven Lavelle：Motion Planning Algorithms
Principles of Robot Motion：Theory, Algorithms, and Implementations
Gonzalez, David, IEEE: A Review of Motion Planning Techniques for Automated Vehicles
补充：后面的课程是对这个overview的扩充，但讲解比较粗糙，ppt都是论文截图，不易懂

6 百度Apollo ROS介绍
课程讲解ROS的定义、原理以及Apollo对ROS的优化。
6.1 背景和ros介绍
（1）介绍了ros的基本概念，编译和常用命令，跟mfr一样，最后提到了基于ros的Gazebo simulator仿真工具
补充：https://developer.apollo.auto/devcenter/courseplaying_cn.html?target=2-251-297 ，这节课程不错，可以多次观看
补充：ros非常成熟，有一套成熟的资料：E:\自动驾驶学习\图片\智能驾驶进阶课程（百度）\Ros资料
6.2 apollo ros原理
（1）描述apollo对ros的三大优化：
通信性能优化：使用共享内存替换socket进行跨进程通信，效率高。目前mfr已经采用，持续完善中！
去中心化网络拓扑：之前是树状的拓扑，节点建立连接依赖master节点，如果中心节点故障，整个系统稳定性不保！依赖“RTPS服务发现协议”，apollo去除了中心节点，增加通信域概念domain，域内的多个节点都能负责管理拓扑，实现了网状拓扑。目前mfr还没做这点！！
数据兼容性扩展：深度整合protobuf消息格式，实现后向兼容，且通信过程中不需要额外的序列化和反序列化，减少copy。mfr也用了protobuf，但是传输过程中需要序列化和反序列化，属于传统的ros消息机制，因此也需要升级！
补充：apollo 3.5版本时，把自己基于ros的运行时框架开源并命名为Apollo CyberRT，资料如下：
https://blog.csdn.net/qq_25762163/article/details/103591766?spm=1001.2014.3001.5502
https://github.com/ApolloAuto/apollo/tree/r3.5.0/cyber
（2）TF坐标系转换：作用是将汽车或机器人各个点的各自的坐标统一到一个世界坐标系中；
（3）RQT：即ros的qt工具，可以可视化的显示我们想要的信息，常用的有rqt image（看图像帧），rqt multiplot（画折线图），rqt graph（画拓扑图），rqt console（显示log）
（4）ros time：相当于mtime，取系统时间，但是也有仿真时间功能，即回放时，将时间拉回到录包的时间
（5）还讲了ros bag和rviz（公司后面用mfbag和mviz替换）

7 百度Apollo感知介绍
7.1 感知概述
（1）感知是机器人学科的问题，机器人要学习人类，但又有区别。人类大脑要实时分析大量信息，但自动驾驶系统里，大量信息通过高精地图预制进系统，不需要感知，感知只做最基础的最小集合，保证系统的实时性和鲁棒性；
（2）感知一定是一个软硬件结合的问题，硬件决定软件，做感知需要系统化的思维方式，而深度学习只是它的一个小子集。未来的趋势是传感器融合，降低感知算法的复杂度。
（3）感知目前没有终极方案，一定是个技术长跑，把握准技术方向，做时间的朋友；
7.2 传感器和标定
（1）分析了lidar，radar，camera，uss，高精地图的特点，但还特别提到了一种新的融合传感器：camera+lidar，即同时接收图像+点云，直接输出4D数据，即三维数据+距离，目前有美国的AEye，中国禾赛做这个；
（2）多个传感器进行数据融合的时候，一定要有足够的overlap，即重叠区域，这要求传感器安装的时候就要考虑到，要在设计的时候做好传感器探测距离图！
（3）传感器标定：将不同传感器数据在同一个坐标系里表示，分为：
内参：传感器自身性质，如camera焦距，lidar各激光管的垂直朝向角，一般厂家提供。
外参：传感器之间的相对位置和朝向，用3自由度的旋转矩阵和3自由度的平移向量表示，整车厂自己标
（4）标定的核心思路：让两个传感器看同一个东西，只有有一个确定，另一个就能确定，从而计算相对位置；
（5）常见的标定算法：
lidar内参标定
lidar-to-gps外参标定
lidar-to-lidar外参标定
lidar-to-camera外参标定
camera-to-camera外参标定
camera-to-radar外参标定
自然场景下lidar-to-camera外参标定
自然场景下camera-to-camera外参标定
补充：点云拼接使用ICP算法，标定解方程是PNP问题，具体百度搜！
7.3 感知算法
（1）讲解lidar感知的检测，视觉感知的整个流程，radar感知的检测，uss感知的检测
（2）lidar感知的检测：目标是获取障碍物的位置和大小，以及类别，朝向，轨迹和速度。两种算法：
启发式方法：Ncut，基于空间平滑性假设，构建点云graph，将检测转化为graph分割聚类问题。解释性好，但难以应对实际大量场景。
Deep learning（DL）方法：CNNSeg，CNNSeg由apollo首创，已经开源，具体见论文：Vehicle Detection from 3D Lidar Using Fully Convolutional Network
补充：启发式方法便于我们深入分析和理解问题，一定程度来说是DL方法的来源，不可跳过。
补充：一种融合了图像的点云感知算法：MV3D，把lidar测距准和camera识别准结合起来，见论文：Multi-View 3D Object Detection Network for Autonomous Driving
（3）视觉感知：从视觉ADAS发展而来，标准公司是Mobileye，
检测和分割是一类问题，都是标识物体，只是粗细粒度问题，采用DL的CNN做；
2d-to-3d，tarcking跟踪和多相机融合属于后处理部分，要求轻量快捷；
补充：视觉检测可以参考何恺明的工作，不过他专注于Computer Vision（CV）的检测，与Autonomous Driving（AD）的检测存在一定的区别
补充：视觉感知发展的方向是将后处理也由DL做，一个model处理整个过程
（4）视觉感知识别红路灯：要求精度达到3个9（将近100%），各种天气光照下都能识别，各种样子的红路灯都能识别。技术方式是采用视觉感知和高精地图配合，高精地图提供红路灯坐标和交通含义，为感知提供ROI，感知只负责辨识颜色。
（5）radar感知和uss感知没怎么讲。
补充：lidar感知像是盲人拿棍走路，视觉感知像正常人，目前看lidar的效果更好，长远看视觉感知是方向！
7.4 感知中的机器学习（提出几个开放问题）
（1）自动驾驶感知对准确率，召回率和延时要求很苛刻，说白了就是又快又准，需要软硬件共同配合，共同发展！
（2）机器学习的训练集和测试集是相互独立的，前者封闭，后者开放，因此容易测出corner case。目前的量产思路是限制使用场景，并逐步扩大！
（3）无人车一旦遇到corner case，可解释性是否必须？不像启发式算法，DL会掩盖问题的本质，但只要这个case通过了测试，可解释性就没那么重要了。
（4）深度学习不能解决所有问题，例如常识性问题，公理性问题，总要与其它方法配合！
（5）有监督学习太弱了，进化的方向一定是无监督学习，让无人车像人那样同时做预测和学习。
7.5 感知未来
（1）sensor迭代非常快，如量产激光雷达
（2）DL+仿真数据+车载AI专用芯片，能让无人车感知加速跑起来
（3）智能交通设施，尤其是v2x普及，能让车省去很多感知需求，比如红路灯
（4）人工智能技术进一步提升
7.6 几个课后问题，帮助深入理解感知
（1）前向和后向的安全感知距离应如何计算？
（2）如何计算感知延时对自动驾驶汽车安全行驶的影响？
（3）调研量产车自适应循环ACC的实现原理？
（4）推导相机pitch角对障碍物距离估计的影响，0.1度误差会有怎样的影响？

8 百度Apollo控制介绍
课程讲解控制理论以及控制技术在Apollo无人驾驶中的应用、控制技术在目前无人车方案中的限制以及未来的发展，控制技术与无人车其他技术模块的联动。
（1）讲解过于粗糙，又涉及大量运算，建议参考入门课程中的控制部分

9 百度Apollo安装和仿真平台
课程讲解Apollo作为自动驾驶平台，学习者如何从硬件、软件等方面快速入门。同时，课程还会介绍Apollo的部署安装以及仿真平台的使用。
（1）apollo官网提供docker镜像，帮助开发者快速搭建pc环境，并使用dreamview仿真工具查看测试效果，具体安装过程见：
https://developer.apollo.auto/devcenter/courseplaying_cn.html?target=2-254-283
（2）apollo官网提供仿真平台，开发者可以尝试使用：
https://developer.apollo.auto/devcenter/courseplaying_cn.html?target=2-255-275
